---
$title@: MathML
description@: 'Read about MathML'
$order: 8
---

{% do doc.amp_dependencies.add('amp-mathml', '0.1') %}
<section>

<div class="math-header">The Quadratic Formula</div>
<amp-mathml layout="container" data-formula="\[x = {-b \pm \sqrt{b^2-4ac} \over 2a}\]"></amp-mathml>

<div class="math-header">Cauchy's Integral Formula</div>
<amp-mathml layout="container" data-formula="\[f(a) = \frac{1}{2\pi i} \oint _γ \frac{f(z)}{z-a}dz\]"></amp-mathml>

<div class="math-header">Double angle formula for Cosines</div>
<amp-mathml layout="container" data-formula="$$ \cos(θ+φ)=\cos(θ)\cos(φ)−\sin(θ)\sin(φ) $$"></amp-mathml>

<div class="math-header">Gauss' Divergence Theorem</div>
<amp-mathml layout="container" data-formula="\[\int_D(∇⋅F)dV = \int_{∂D} F⋅ndS\]"></amp-mathml>

<div class="math-header">Definition of Christoffel Symbols</div>
<amp-mathml layout="container" data-formula="\[(∇_XY)^k = X^i(∇_iY)^k = X^i{\left({∂Y^k \over ∂x^i} + Γ_{im}^k Y^m\right)}\]"></amp-mathml>

<div class="math-header">Curl of a Vector Field</div>
<amp-mathml layout="container" data-formula="\[\overrightarrow{\nabla} × \overrightarrow{F} = \left(\frac{∂F_z}{∂y} − \frac{∂F_y}{∂z}\right)i + \left(\frac{∂F_x}{∂z} − \frac{∂F_z}{∂x}\right)j + \left(\frac{∂F_y}{∂x} − \frac{∂F_x}{∂y}\right)k\]"></amp-mathml>

<div class="math-header">Standard Deviation</div>
<amp-mathml layout="container" data-formula="\[σ = \sqrt{\frac{1}{N}\sum_{i=1}^N(x_i−μ)^2}\]"></amp-mathml>

<div class="math-header">Stephen Hawking's Entropy of a Black Hole</div>
<amp-mathml layout="container" data-formula="\[S = {kc^3A \over 4ℏG}\]"></amp-mathml>

<p>
Symbols from Wikipedia
</p>
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[\sqsubseteq \top \bot \sqcap \sqcup \neg \forall \exists \sqsubseteq \equiv\]
"></amp-mathml>
</span>


<p>
Let
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[\{x^1, x^2, &#8230;, x^N\}\]
"></amp-mathml>
</span>
&#x0020;
be the set of ingredient phrases, e.g. {&#8220;&#xbd; cups whole wheat flour&#8221;, &#8220;pinch of salt&#8221;, &#8230;} where each
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[x^i\]
"></amp-mathml>
</span>
&#x0020;
is an ordered list of words. Associated with each
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[x^i\]
"></amp-mathml>
</span>
&#x0020;
is a list of tags,
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[y^i\]
"></amp-mathml>.
</span>
</p>

<p>
For example, if
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[x^i = &#x005B;x_1^i, x_2^i, x_3^i&#x005D; = &#x005B;&#8220;pinch&#8221;, &#8220;of&#8221;, &#8220;salt&#8221;&#x005D;\]
"></amp-mathml>
</span>
&#x0020;
then
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[y^i = &#x005B;y_1^i, y_2^i, y_3^i&#x005D; = &#x005B;UNIT, UNIT, NAME&#x005D;\]
"></amp-mathml>.
</span>
&#x0020;
A tag is either a NAME, UNIT, QUANTITY, COMMENT or OTHER (i.e., none of the above).
</p>

<p>
The goal is to use data to learn a model that can predict the tag sequence for any ingredient phrase we throw at it, even if the model has never seen that ingredient phrase before. We approach this task by modeling the conditional probability of a sequence of tags given the input, denoted
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[p(TagSequence \mid IngredientPhrase) \]
"></amp-mathml>
&#x0020;
or using the above notation,
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[p(y \mid x) \]
"></amp-mathml>.
</span>
</p>

<p>
The process of learning that probability model is described in detail below, but first imagine that someone handed us the perfect probability model
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[p(y \mid x) \]
"></amp-mathml>
</span>
&#x0020;
that returns the &#8220;true&#8221; probability of a sequence of labels given an ingredient phrase. We want to use
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[p(y \mid x) \]
"></amp-mathml>
</span>
&#x0020;
to discover (or <i>infer</i>) the most probable label sequence.
</p>

<p>
Armed with this model, we could predict the best sequence of labels for an ingredient phrase by simply searching over all tag sequences and returning the one that has the highest probability.
</p>

<p>
For example, suppose our ingredient phrase is &#8220;pinch of salt.&#8221; Then we need to score all the possible sequences of 3 tags.
</p>


<div class="row">
<div class="col-sm-10">
<span class="amp-formula">
<amp-mathml
  layout="container"
  data-formula="
  \[\huge \exists \: \exists! \: \nexists \: \forall \neg \lor \div \land \implies \Rightarrow \Longleftarrow \Leftarrow \iff \Leftrightarrow \]
"></amp-mathml>
</span>
</div>
</div>

<div class="row">
<div class="col-sm-10">
<span class="amp-formula">
<amp-mathml
  layout="container"
  data-formula="
  \[\huge \varnothing \mathbb{N} \mathbb{Z} \mathbb{Q} \mathbb{A} \mathbb{R} \mathbb{C} ]\in \notin \ni \subset \subseteq \supset \supseteq \cup \cap \]
"></amp-mathml>
</span>
</div>
</div>


<div class="row">
<div class="col-sm-10">
<span class="amp-formula">
<amp-mathml
  layout="container"
  data-formula="
  \[p(\text{UNIT QUANTITY QUANTITY} \mid \text{&#8220;pinch of salt&#8221;}) \]
"></amp-mathml>
</span>
</div>
</div>

<div class="row">
<div class="col-sm-10">
<span class="amp-formula">
<amp-mathml
  layout="container"
  data-formula="
  \[\vdots\]
"></amp-mathml>
</span>
</div>
</div>

<p>
While this seems like a simple problem, it can quickly become computationally unpleasant to score all of the
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[\text{tags} \mid ^{\mid\text{words}\mid}\]
"></amp-mathml>
</span>
&#x0020;
sequences. The beauty of the linear&#x002D;chain CRF model is that it makes some conditional independence assumptions that allow us to use
&#x0020;
<a class="underline" href="//en.wikipedia.org/wiki/Viterbi_algorithm">dynamic programming</a>
&#x0020;
to efficiently search the space of all possible label sequences. In the end, we are able to find the best tag sequence in a time that is quadratic in the number of tags and linear in the number of words
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[(\mid\text{tags}\mid^2*\mid\text{words}\mid)\]
"></amp-mathml>.
</span>
</p>

<p>
So given a model
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[p(y \mid x)\]
"></amp-mathml>
</span>
&#x0020;
that encodes whether a particular tag sequence is a good fit for a ingredient phrase, we can return the best tag sequence. But how do we learn that model?
</p>

<p>
A linear&#x002D;chain CRF models this probability in the following way:
</p>

<p>
<span class="amp-formula">
<amp-mathml
  layout="container"
  data-formula="
  \begin{equation}
   p( y \mid x ) \propto \prod_{t=1}^T \psi(y_t, y_{t-1}, x)
  \end{equation}
"></amp-mathml>
</span>
</p>

<p>
where
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[T\]
"></amp-mathml>
</span>
&#x0020;
is the number of words in the ingredient phrase
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[x\]
"></amp-mathml>
</span>
</p>

<p>
Let&#x2019;s break this equation down in English.
</p>

<p>
The above equation introduces a &#8220;potential&#8221; function
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[\psi\]
"></amp-mathml>
</span>
&#x0020;
that takes two consecutive labels,
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[y_t\]
"></amp-mathml>
</span>
&#x0020;
and
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[y_{t-1}\]
"></amp-mathml>
</span>
&#x0020;
, and the ingredient phrase,
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[x\]
"></amp-mathml>
</span>
&#x0020;
. We construct
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[\psi\]
"></amp-mathml>
</span>
&#x0020;
so that it returns a large, non&#x002D;negative number if the labels
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[y_t\]
"></amp-mathml>
</span>
&#x0020;
and
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[y_{t-1}\]
"></amp-mathml>
</span>
&#x0020;
are a good match for the
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[t^{th}\]
"></amp-mathml>
</span>
&#x0020;
and
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[{t-1}^{th}\]
"></amp-mathml>
</span>
&#x0020;
words in the sentence respectively, and a small, non&#x002D;negative number if not. (Remember that probabilities must be non&#x002D;negative.)
</p>

<p>
The potential function is a weighted average of simple feature functions, each of which captures a single attribute of the labels and words.
</p>

<p>
<span class="amp-formula">
<amp-mathml
  layout="container"
  data-formula="
  \begin{equation}
   \psi(y_t, y_{t-1}, x) = \exp{\sum_{k=1}^K w_k f_k(y_t, y_{t-1}, x)}
  \end{equation}
"></amp-mathml>
</span>
</p>

<p>
We often define feature functions to return either 0 or 1. Each feature function,
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[f_k(y_t, y_{t-1}, x)\]
"></amp-mathml>
</span>
, is chosen by the person who creates the model, based on what information might be useful to determine the relationship between words and labels. Some feature functions we used for this problem were:
</p>

<p>
<span class="amp-formula">
<amp-mathml
 layout="container"
 data-formula="
  \begin{align*}
   &#038;f_1(y_t, y_{t-1}, x) = \left\{
   \begin{array}{lr}
    1 \text{ if } x_t \text{ is capitalized and }y_t \text{ is NAME} \\
    0 \text{ otherwise}
   \end{array}
   \right.\\ \\
   &#038;f_2(y_t, y_{t-1}, x) = \left\{
   \begin{array}{lr}
    1 \text{ if } x_t \text{ is &#8220;1/2&#8221; and } y_t \text{ is QUANTITY} \\
    0 \text{ otherwise}
   \end{array} \right. \\ \\
   &#038;f_3(y_t, y_{t-1}, x) = \left\{
   \begin{array}{lr}
    1 \text{ if } x_t \text{ is &#8220;cup&#8221; and }y_t \text{is QUANTITY} \\
    0 \text{ otherwise}
    \end{array} \right.\\ \\
   &#038;f_4(y_t, y_{t-1}, x) = \left\{
   \begin{array}{lr}
    1 \text{ if } x_t \text{ is &#8220;flour&#8221; and } y_t \text{ is QUANTITY} \\
    0 \text{ otherwise}
   \end{array} \right.\\ \\
   &#038;f_5(y_t, y_{t-1}, x) = \left\{
   \begin{array}{lr}
    1 \text{ if } x_t \text{ is a fraction and } y_t \text{is QUANTITY} \\
    0 \text{ otherwise}
   \end{array} \right.\\ \\
   &#038;f_6(y_t, y_{t-1}, x) = \left\{
    \begin{array}{lr}
    1 \text{ if } y_t \text{ is QUANTITY and } y_{t-1} \text{is UNIT} \\
    0 \text{ otherwise}
   \end{array} \right.\\ \\
   &#038;f_7(y_t, y_{t-1}, x) = \left\{
   \begin{array}{lr}
    1 \text{ if } y_t \text{ is QUANTITY and } y_{t-1} \text{ is NAME} \\
    0 \text{ otherwise}
   \end{array} \right.\\
  \end{align*}
"></amp-mathml>
</span>
</p>

<p>
There is a feature function for every word&#x002F;label pair and for every consecutive label pair, plus some hand&#x002D;crafted functions. By modeling the conditional probability of labels given words the following way, we have reduced our task of learning
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[p(y \mid x) \]
"></amp-mathml>
</span>
&#x0020;
to the problem of learning &#8220;good&#8221; weights on each of the feature functions. By good, we mean that we want to learn large positive weights on features that capture highly likely patterns in the data, large negative weights on features that capture highly unlikely patterns in the data and small weights on features that don&#x2019;t capture any patterns in the data.
</p>

<p>
For example,
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[f2\]
"></amp-mathml>
</span>
&#x0020;
describes a likely pattern in the data good (&#8220;&#xbd;&#8221; is likely a quantity),
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[f4\]
"></amp-mathml>
</span>
&#x0020;
describes an unlikely pattern in the data (the word &#8220;flour&#8221; is almost never a quantity) and
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[f1\]
"></amp-mathml>
</span>
&#x0020;
doesn&#x2019;t capture a common pattern (the ingredient phrases are almost always lowercased). In this case, we want
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[w2\]
"></amp-mathml>
</span>
&#x0020;
to be a large positive number,
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[w4\]
"></amp-mathml>
</span>
&#x0020;
to be a large negative number and
&#x0020;
<span class="amp-formula">
<amp-mathml
  layout="container"
  inline=""
  data-formula="
  \[w1\]
"></amp-mathml>
</span>
&#x0020;
to be close to 0.
</p>


</section>
