---
$title@: 'amp-library mathml'
description@: 'Read about Reverse Engineering Recipes'
$order: 2
author: jaygray0919
contributors:
  - gigster99
---

{% do doc.amp_dependencies.add('amp-mathml', '0.1') %}
<section>

<p>
One challenging aspect of the recipe parsing problem is the task of predicting ingredient components from the ingredient phrases. Recipes display ingredients like &#8220;1 tablespoon fresh lemon juice,&#8221; but the database stores ingredients broken down by name (&#8220;lemon juice&#8221;), quantity (&#8220;1&#8221;), unit (&#8220;tablespoon&#8221;) and comment (&#8220;fresh&#8221;). There is no regular expression clever enough to identify these labels from the ingredient phrases.
</p>

<h4>Example, Ingredient Labels</h4>

<div class='table-responsive'>
<table class='table-inverse'>
<thead>
<tr>
<th></th>
<th>QUANTITY</th>
<th>UNIT</th>
<th>COMMENT</th>
<th>NAME</th>
<th>NAME</th>
</tr>
</thead>

<tbody>
<tr>
<td>Ingredient Phrase</td>
<td>1</td>
<td>tablespoon</td>
<td>fresh</td>
<td>lemon</td>
<td>juice</td>
</tr>
</tbody>
</table>
</div>

<p>
This type of problem is referred to as a
&#x0020;
<a class='underline' href='http://www.cs.cmu.edu/~nasmith/sp4nlp/'>structured prediction</a>
&#x0020;
problem because we are trying to predict a structure &#x2014; in this case a sequence of labels &#x2014; rather than a single label. Structured prediction tasks are difficult because the choice of a particular label for any one word in the phrase can change the model&#x2019;s prediction of labels for the other words. The added model complexity allows us to learn rich patterns about how words in a sentence interact with the words and labels around them.
</p>

<p>
We chose to use a discriminative structured prediction model called a
&#x0020;
<a class='underline' href='https://homepages.inf.ed.ac.uk/csutton/publications/crftutv2.pdf'>linear&#x002D;chain conditional random field</a>
&#x0020;
(CRF), which has been successful on similar tasks such as part&#x2010;of&#x2010;speech tagging and named entity recognition.
</p>

<p>
The basic set up of the problem is as follows:
</p>

<p>
Let
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[\{x^1, x^2, &#8230;, x^N\}\]
'></amp-mathml>
</span>
&#x0020;
be the set of ingredient phrases, e.g. {&#8220;&#xbd; cups whole wheat flour&#8221;, &#8220;pinch of salt&#8221;, &#8230;} where each
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[x^i\]
'></amp-mathml>
</span>
&#x0020;
is an ordered list of words. Associated with each
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[x^i\]
'></amp-mathml>
</span>
&#x0020;
is a list of tags,
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[y^i\]
'></amp-mathml>.
</span>
</p>

<p>
For example, if
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[x^i = &#x005B;x_1^i, x_2^i, x_3^i&#x005D; = &#x005B;&#8220;pinch&#8221;, &#8220;of&#8221;, &#8220;salt&#8221;&#x005D;\]
'></amp-mathml>
</span>
&#x0020;
then
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[y^i = &#x005B;y_1^i, y_2^i, y_3^i&#x005D; = &#x005B;UNIT, UNIT, NAME&#x005D;\]
'></amp-mathml>.
</span>
&#x0020;
A tag is either a NAME, UNIT, QUANTITY, COMMENT or OTHER (i.e., none of the above).
</p>

<p>
The goal is to use data to learn a model that can predict the tag sequence for any ingredient phrase we throw at it, even if the model has never seen that ingredient phrase before. We approach this task by modeling the conditional probability of a sequence of tags given the input, denoted
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[p(TagSequence \mid IngredientPhrase) \]
'></amp-mathml>
&#x0020;
or using the above notation,
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[p(y \mid x) \]
'></amp-mathml>.
</span>
</p>

<p>
The process of learning that probability model is described in detail below, but first imagine that someone handed us the perfect probability model
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[p(y \mid x) \]
'></amp-mathml>
</span>
&#x0020;
that returns the &#8220;true&#8221; probability of a sequence of labels given an ingredient phrase. We want to use
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[p(y \mid x) \]
'></amp-mathml>
</span>
&#x0020;
to discover (or <i>infer</i>) the most probable label sequence.
</p>

<p>
Armed with this model, we could predict the best sequence of labels for an ingredient phrase by simply searching over all tag sequences and returning the one that has the highest probability.
</p>

<p>
For example, suppose our ingredient phrase is &#8220;pinch of salt.&#8221; Then we need to score all the possible sequences of 3 tags.
</p>

<div class='row'>
<div class='col-sm-10'>
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 data-formula='
	 \[p(\text{UNIT UNIT UNIT} \mid \text{&#8220;pinch of salt&#8221;}) \]
'></amp-mathml>
</span>
</div>
</div>
<div class='row'>
<div class='col-sm-10'>
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 data-formula='
	 \[p(\text{QUANTITY UNIT UNIT} \mid \text{&#8220;pinch of salt&#8221;}) \]
'></amp-mathml>
</span>
</div>
</div>
<div class='row'>
<div class='col-sm-10'>
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 data-formula='
	 \[p(\text{UNIT QUANTITY UNIT} \mid \text{&#8220;pinch of salt&#8221;}) \]
'></amp-mathml>
</span>
</div>
</div>
<div class='row'>
<div class='col-sm-10'>
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 data-formula='
	 \[p(\text{UNIT UNIT QUANTITY} \mid \text{&#8220;pinch of salt&#8221;}) \]
'></amp-mathml>
</span>
</div>
</div>
<div class='row'>
<div class='col-sm-10'>
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 data-formula='
	 \[p(\text{UNIT QUANTITY QUANTITY} \mid \text{&#8220;pinch of salt&#8221;}) \]
'></amp-mathml>
</span>
</div>
</div>
<div>
<div class='row'>
<div class='col-sm-10'>
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 data-formula='
	 \[p(\text{QUANTITY QUANTITY QUANTITY} \mid \text{&#8220;pinch of salt&#8221;}) \]
'></amp-mathml>
</span>
</div>
</div>
<div class='row'>
<div class='col-sm-10'>
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 data-formula='
	 \[p(\text{UNIT QUANTITY NAME} \mid \text{&#8220;pinch of salt&#8221;}) \]
'></amp-mathml>
</span>
</div>
</div>
<div class='row'>
<div class='col-sm-10'>
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 data-formula='
	 \[\vdots\]
'></amp-mathml>
</span>
</div>
</div>

<p>
While this seems like a simple problem, it can quickly become computationally unpleasant to score all of the
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[\text{tags} \mid ^{\mid\text{words}\mid}\]
'></amp-mathml>
</span>
&#x0020;
sequences. The beauty of the linear&#x002D;chain CRF model is that it makes some conditional independence assumptions that allow us to use
&#x0020;
<a class='underline' href='//en.wikipedia.org/wiki/Viterbi_algorithm'>dynamic programming</a>
&#x0020;
to efficiently search the space of all possible label sequences. In the end, we are able to find the best tag sequence in a time that is quadratic in the number of tags and linear in the number of words
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[(\mid\text{tags}\mid^2*\mid\text{words}\mid)\]
'></amp-mathml>.
</span>
</p>

<p>
So given a model
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[p(y \mid x)\]
'></amp-mathml>
</span>
&#x0020;
that encodes whether a particular tag sequence is a good fit for a ingredient phrase, we can return the best tag sequence. But how do we learn that model?
</p>

<p>
A linear&#x002D;chain CRF models this probability in the following way:
</p>

<p>
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 data-formula='
		\begin{equation}
			p( y \mid x ) \propto \prod_{t=1}^T \psi(y_t, y_{t-1}, x)
		\end{equation}
'></amp-mathml>
</span>
</p>

<p>
where
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[T\]
'></amp-mathml>
</span>
&#x0020;
is the number of words in the ingredient phrase
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[x\]
'></amp-mathml>
</span>
</p>

<p>
Let&#x2019;s break this equation down in English.
</p>

<p>
The above equation introduces a &#8220;potential&#8221; function
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[\psi\]
'></amp-mathml>
</span>
&#x0020;
that takes two consecutive labels,
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[y_t\]
'></amp-mathml>
</span>
&#x0020;
and
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[y_{t-1}\]
'></amp-mathml>
</span>
&#x0020;
, and the ingredient phrase,
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[x\]
'></amp-mathml>
</span>
&#x0020;
. We construct
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[\psi\]
'></amp-mathml>
</span>
&#x0020;
so that it returns a large, non&#x002D;negative number if the labels
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[y_t\]
'></amp-mathml>
</span>
&#x0020;
and
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[y_{t-1}\]
'></amp-mathml>
</span>
&#x0020;
are a good match for the
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[t^{th}\]
'></amp-mathml>
</span>
&#x0020;
and
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[{t-1}^{th}\]
'></amp-mathml>
</span>
&#x0020;
words in the sentence respectively, and a small, non&#x002D;negative number if not. (Remember that probabilities must be non&#x002D;negative.)
</p>

<p>
The potential function is a weighted average of simple feature functions, each of which captures a single attribute of the labels and words.
</p>

<p>
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 data-formula='
		\begin{equation}
			\psi(y_t, y_{t-1}, x) = \exp{\sum_{k=1}^K w_k f_k(y_t, y_{t-1}, x)}
		\end{equation}
'></amp-mathml>
</span>
</p>

<p>
We often define feature functions to return either 0 or 1. Each feature function,
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[f_k(y_t, y_{t-1}, x)\]
'></amp-mathml>
</span>
, is chosen by the person who creates the model, based on what information might be useful to determine the relationship between words and labels. Some feature functions we used for this problem were:
</p>

<p>
<span class='amp-formula'>
<amp-mathml
	layout='container'
	data-formula='
		\begin{align*}
			&#038;f_1(y_t, y_{t-1}, x) = \left\{
			\begin{array}{lr}
				1 \text{ if } x_t \text{ is capitalized and }y_t \text{ is NAME} \\
				0 \text{ otherwise}
			\end{array}
			\right.\\ \\
			&#038;f_2(y_t, y_{t-1}, x) = \left\{
			\begin{array}{lr}
				1 \text{ if } x_t \text{ is &#8220;1/2&#8221; and } y_t \text{ is QUANTITY} \\
				0 \text{ otherwise}
			\end{array} \right. \\ \\
			&#038;f_3(y_t, y_{t-1}, x) = \left\{
			\begin{array}{lr}
				1 \text{ if } x_t \text{ is &#8220;cup&#8221; and }y_t \text{is QUANTITY} \\
				0 \text{ otherwise}
				\end{array} \right.\\ \\
			&#038;f_4(y_t, y_{t-1}, x) = \left\{
			\begin{array}{lr}
				1 \text{ if } x_t \text{ is &#8220;flour&#8221; and } y_t \text{ is QUANTITY} \\
				0 \text{ otherwise}
			\end{array} \right.\\ \\
			&#038;f_5(y_t, y_{t-1}, x) = \left\{
			\begin{array}{lr}
				1 \text{ if } x_t \text{ is a fraction and } y_t \text{is QUANTITY} \\
				0 \text{ otherwise}
			\end{array} \right.\\ \\
			&#038;f_6(y_t, y_{t-1}, x) = \left\{
				\begin{array}{lr}
				1 \text{ if } y_t \text{ is QUANTITY and } y_{t-1} \text{is UNIT} \\
				0 \text{ otherwise}
			\end{array} \right.\\ \\
			&#038;f_7(y_t, y_{t-1}, x) = \left\{
			\begin{array}{lr}
				1 \text{ if } y_t \text{ is QUANTITY and } y_{t-1} \text{ is NAME} \\
				0 \text{ otherwise}
			\end{array} \right.\\
		\end{align*}
'></amp-mathml>
</span>
</p>

<p>
There is a feature function for every word&#x002F;label pair and for every consecutive label pair, plus some hand&#x002D;crafted functions. By modeling the conditional probability of labels given words the following way, we have reduced our task of learning
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[p(y \mid x) \]
'></amp-mathml>
</span>
&#x0020;
to the problem of learning &#8220;good&#8221; weights on each of the feature functions. By good, we mean that we want to learn large positive weights on features that capture highly likely patterns in the data, large negative weights on features that capture highly unlikely patterns in the data and small weights on features that don&#x2019;t capture any patterns in the data.
</p>

<p>
For example,
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[f2\]
'></amp-mathml>
</span>
&#x0020;
describes a likely pattern in the data good (&#8220;&#xbd;&#8221; is likely a quantity),
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[f4\]
'></amp-mathml>
</span>
&#x0020;
describes an unlikely pattern in the data (the word &#8220;flour&#8221; is almost never a quantity) and
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[f1\]
'></amp-mathml>
</span>
&#x0020;
doesn&#x2019;t capture a common pattern (the ingredient phrases are almost always lowercased). In this case, we want
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[w2\]
'></amp-mathml>
</span>
&#x0020;
to be a large positive number,
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[w4\]
'></amp-mathml>
</span>
&#x0020;
to be a large negative number and
&#x0020;
<span class='amp-formula'>
<amp-mathml
	 layout='container'
	 inline=''
	 data-formula='
	 \[w1\]
'></amp-mathml>
</span>
&#x0020;
to be close to 0.
</p>

<p>
Due to properties of the model &#x2014; chiefly, that the function is convex with respect to the weights &#x2014; there is one best set of weights and we can find it using an iterative optimization
&#x0020;
<a class='underline' href='https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm'>algorithm</a>
. We used the
&#x0020;
<a class='underline' href='https://taku910.github.io/crfpp/'>CRF++</a>
&#x0020;
implementation to do the optimization and inference.
</p>

</section>
